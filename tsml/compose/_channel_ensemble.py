# -*- coding: utf-8 -*-
"""ChannelEnsembleClassifier: For Multivariate Time Series Classification.

Builds classifiers on each dimension (channel) independently.
"""

__author__ = ["abostrom", "MatthewMiddlehurst"]
__all__ = ["ChannelEnsembleClassifier", "ChannelEnsembleRegressor"]

from abc import ABCMeta

import numpy as np
import pandas as pd
from sklearn.base import ClassifierMixin, RegressorMixin
from sklearn.utils.multiclass import check_classification_targets
from sklearn.utils.validation import check_is_fitted, check_random_state

from tsml.base import BaseTimeSeriesEstimator, _clone_estimator


class _BaseChannelEnsemble(BaseTimeSeriesEstimator, metaclass=ABCMeta):
    def __init__(self, estimators, remainder, random_state):
        self.estimators = estimators
        self.remainder = remainder
        self.random_state = random_state

        super(_BaseChannelEnsemble, self).__init__()

    _required_parameters = ["estimators"]

    def _validate_estimators(self, required_predict_method="predict"):
        if self.estimators is None or len(self.estimators) == 0:
            raise AttributeError(
                "Invalid estimators attribute, estimators should be a list of "
                "(string, estimator, dimensions) tuples"
            )

        names, estimators, _ = zip(*self.estimators)

        self._check_names(names)

        # validate estimators
        for t in estimators:
            if t == "drop":
                continue
            if not hasattr(t, "fit") or not hasattr(t, required_predict_method):
                raise TypeError(
                    "All estimators should implement fit and "
                    f"{required_predict_method}, or can be 'drop' specifiers. '{t}' "
                    f"(type {type(t)}) doesn't."
                )

    def _check_names(self, names):
        if len(set(names)) != len(names):
            raise ValueError(f"Names provided are not unique: {list(names)}")

        invalid_names = set(names).intersection(self.get_params(deep=False))
        if invalid_names:
            raise ValueError(
                "Estimator names conflict with constructor arguments: "
                f"{sorted(invalid_names)}"
            )

        invalid_names = [name for name in names if "__" in name]
        if invalid_names:
            raise ValueError(
                f"Estimator names must not contain __: got f{invalid_names}"
            )

    def _validate_channels(self, X):
        """Convert callable channel specifications."""
        channels = []
        for _, _, channel in self.estimators:
            if callable(channel):
                channel = channel(X)
            if channel == "all":
                channel = list(range(X[0].shape[0]))
            channels.append(channel)

        self._channels = channels

    def _validate_remainder(self, X):
        """Validate remainder and defines _remainder."""
        is_estimator = hasattr(self.remainder, "fit") and hasattr(
            self.remainder, "predict"
        )
        if self.remainder != "drop" and not is_estimator:
            raise ValueError(
                f"The remainder keyword needs to be 'drop', {self.remainder} was "
                "passed instead"
            )

        n_channels = X.shape[1]
        cols = []
        for channels in self._channels:
            cols.extend(_get_channel_indices(X, channels))
        remaining_idx = sorted(list(set(range(n_channels)) - set(cols))) or None

        self._remainder = ("remainder", self.remainder, remaining_idx)

    def _get_estimators(self):
        """Generate (name, estimator, channel) tuples."""
        estimators = [
            (name, estimator, channel)
            for (name, estimator, _), channel in zip(self.estimators, self._channels)
        ]

        # add tuple for remainder
        if self._remainder[2] is not None:
            estimators.append(self._remainder)

        for name, estimator, channel in estimators:
            if estimator == "drop" or _is_empty_channel_selection(channel):
                continue
            yield name, estimator, channel


class ChannelEnsembleClassifier(ClassifierMixin, _BaseChannelEnsemble):
    """Applies estimators to channels of an array.

    This estimator allows different channels or channel subsets of the input
    to be transformed separately and the features generated by each
    transformer will be ensembled to form a single output.

    Parameters
    ----------
    estimators : list of tuples
        List of (name, estimator, channel(s)) tuples specifying the transformer
        objects to be applied to subsets of the data.

        name : string
            Like in Pipeline and FeatureUnion, this allows the
            transformer and its parameters to be set using ``set_params`` and searched
            in grid search.
        estimator :  or {'drop'}
            Estimator must support `fit` and `predict_proba`. Special-cased
            strings 'drop' and 'passthrough' are accepted as well, to
            indicate to drop the channels.
        channels(s) : array-like of int, slice, boolean mask array. Integer channels
        are indexed from 0? "all"
    remainder : "drop" or estimator, default "drop"
        By default, only the specified channels in `transformations` are
        transformed and combined in the output, and the non-specified
        channels are dropped. (default of ``'drop'``).
        By setting ``remainder`` to be an estimator, the remaining
        non-specified channels will use the ``remainder`` estimator. The
        estimator must support `fit` and `transform`.
    """

    def __init__(self, estimators, remainder="drop", random_state=None):
        super(ChannelEnsembleClassifier, self).__init__(
            estimators, remainder, random_state
        )

    def fit(self, X, y):
        """Fit all estimators, fit the data.

        Parameters
        ----------
        X : 3D np.array of shape = [n_instances, n_dimensions, series_length]

        y : array-like, shape = [n_instances]
            The class labels.

        """
        X, y = self._validate_data(X=X, y=y, ensure_min_samples=2)
        X = self._convert_X(X)

        check_classification_targets(y)

        self.n_instances_, self.n_dims_, self.series_length_ = X.shape
        self.classes_ = np.unique(y)
        self.n_classes_ = self.classes_.shape[0]
        self.class_dictionary_ = {}
        for index, class_val in enumerate(self.classes_):
            self.class_dictionary_[class_val] = index

        if self.n_classes_ == 1:
            return self

        self._validate_estimators(required_predict_method="predict_proba")
        self._validate_channels(X)
        self._validate_remainder(X)

        rng = check_random_state(self.random_state)

        estimators_ = []
        for name, estimator, channel in self._get_estimators():
            estimator = _clone_estimator(estimator, random_state=rng)
            estimator.fit(_get_channel(X, channel), y)
            estimators_.append((name, estimator, channel))

        self.estimators_ = estimators_
        return self

    def predict(self, X) -> np.ndarray:
        check_is_fitted(self)

        # treat case of single class seen in fit
        if self.n_classes_ == 1:
            return np.repeat(list(self.class_dictionary_.keys()), X.shape[0], axis=0)

        return np.array(
            [self.classes_[int(np.argmax(prob))] for prob in self.predict_proba(X)]
        )

    def predict_proba(self, X) -> np.ndarray:
        """Predict class probabilities for X using 'soft' voting."""
        check_is_fitted(self)

        # treat case of single class seen in fit
        if self.n_classes_ == 1:
            return np.repeat([[1]], X.shape[0], axis=0)

        X = self._validate_data(X=X, reset=False)
        X = self._convert_X(X)

        probas = np.asarray(
            [
                estimator.predict_proba(_get_channel(X, channel))
                for (_, estimator, channel) in self.estimators_
            ]
        )

        return np.average(probas, axis=0)

    def _more_tags(self):
        return {
            "X_types": ["np_list", "3darray"],
        }

    @classmethod
    def get_test_params(cls, parameter_set="default"):
        """Return testing parameter settings for the estimator.

        Parameters
        ----------
        parameter_set : str, default="default"
            Name of the set of test parameters to return, for use in tests. If no
            special parameters are defined for a value, will return `"default"` set.
            ChannelEnsembleClassifier provides the following special sets:
                 "results_comparison" - used in some classifiers to compare against
                    previously generated results where the default set of parameters
                    cannot produce suitable probability estimates

        Returns
        -------
        params : dict or list of dict, default={}
            Parameters to create testing instances of the class.
            Each dict are parameters to construct an "interesting" test instance, i.e.,
            `MyClass(**params)` or `MyClass(**params[i])` creates a valid test instance.
            `create_test_instance` uses the first (or only) dictionary in `params`.
        """
        from tsml.interval_based import TSFClassifier

        return {
            "estimators": [
                ("tsf1", TSFClassifier(n_estimators=2), 0),
                ("tsf2", TSFClassifier(n_estimators=2), 0),
            ]
        }


class ChannelEnsembleRegressor(RegressorMixin, _BaseChannelEnsemble):
    """Applies estimators to columns of an array or pandas DataFrame.

    This estimator allows different columns or column subsets of the input
    to be transformed separately and the features generated by each
    transformer will be ensembled to form a single output.

    Parameters
    ----------
    estimators : list of tuples
        List of (name, estimator, column(s)) tuples specifying the transformer
        objects to be applied to subsets of the data.

        name : string
            Like in Pipeline and FeatureUnion, this allows the
            transformer and its parameters to be set using ``set_params`` and searched
            in grid search.
        estimator :  or {'drop'}
            Estimator must support `fit` and `predict_proba`. Special-cased
            strings 'drop' and 'passthrough' are accepted as well, to
            indicate to drop the columns.
        channels(s) : array-like of int, slice, boolean mask array. Integer channels
        are indexed from 0? "all"

    remainder : {'drop', 'passthrough'} or estimator, default 'drop'
        By default, only the specified columns in `transformations` are
        transformed and combined in the output, and the non-specified
        columns are dropped. (default of ``'drop'``).
        By specifying ``remainder='passthrough'``, all remaining columns
        that were not specified in `transformations` will be automatically passed
        through. This subset of columns is concatenated with the output of
        the transformations.
        By setting ``remainder`` to be an estimator, the remaining
        non-specified columns will use the ``remainder`` estimator. The
        estimator must support `fit` and `transform`.
    """

    def __init__(self, estimators, remainder="drop", random_state=None):
        super(ChannelEnsembleRegressor, self).__init__(
            estimators, remainder, random_state
        )

    def fit(self, X, y):
        """Fit all estimators, fit the data.

        Parameters
        ----------
        X : 3D np.array of shape = [n_instances, n_dimensions, series_length]

        y : array-like, shape = [n_instances]
            The class labels.

        """
        X, y = self._validate_data(X=X, y=y, ensure_min_samples=2, y_numeric=True)
        X = self._convert_X(X)

        print(type(X))

        self.n_instances_, self.n_dims_, self.series_length_ = X.shape

        self._validate_estimators()
        self._validate_channels(X)
        self._validate_remainder(X)

        rng = check_random_state(self.random_state)

        estimators_ = []
        for name, estimator, channel in self._get_estimators():
            estimator = _clone_estimator(estimator, random_state=rng)
            estimator.fit(_get_channel(X, channel), y)
            estimators_.append((name, estimator, channel))

        self.estimators_ = estimators_
        return self

    def predict(self, X) -> np.ndarray:
        check_is_fitted(self)

        X = self._validate_data(X=X, reset=False)
        X = self._convert_X(X)

        print(type(X))

        preds = np.asarray(
            [
                estimator.predict(_get_channel(X, channel))
                for (_, estimator, channel) in self.estimators_
            ]
        )

        return np.average(preds, axis=0)

    def _more_tags(self):
        return {
            "X_types": ["np_list", "3darray"],
        }

    @classmethod
    def get_test_params(cls, parameter_set="default"):
        """Return testing parameter settings for the estimator.

        Parameters
        ----------
        parameter_set : str, default="default"
            Name of the set of test parameters to return, for use in tests. If no
            special parameters are defined for a value, will return `"default"` set.
            For classifiers, a "default" set of parameters should be provided for
            general testing, and a "results_comparison" set for comparing against
            previously recorded results if the general set does not produce suitable
            probabilities to compare against.

        Returns
        -------
        params : dict or list of dict, default={}
            Parameters to create testing instances of the class.
            Each dict are parameters to construct an "interesting" test instance, i.e.,
            `MyClass(**params)` or `MyClass(**params[i])` creates a valid test instance.
            `create_test_instance` uses the first (or only) dictionary in `params`.
        """
        from tsml.interval_based import TSFRegressor

        return {
            "estimators": [
                ("tsf1", TSFRegressor(n_estimators=2), 0),
                ("tsf2", TSFRegressor(n_estimators=2), 0),
            ]
        }


def _is_empty_channel_selection(channel):
    """Check if column selection is empty.

    Both an empty list or all-False boolean array are considered empty.
    """
    if hasattr(channel, "dtype") and np.issubdtype(channel.dtype, np.bool_):
        return not channel.any()
    elif hasattr(channel, "__len__"):
        return len(channel) == 0
    else:
        return False


def _get_channel_indices(X, key):
    """
    Get feature channel indices for input data X and key.

    For accepted values of `key`, see the docstring of _get_channel

    """
    n_channels = X.shape[1]

    if (
        _check_key_type(key, int)
        or hasattr(key, "dtype")
        and np.issubdtype(key.dtype, np.bool_)
    ):
        # Convert key into positive indexes
        idx = np.arange(n_channels)[key]
        return np.atleast_1d(idx).tolist()
    elif _check_key_type(key, str):
        try:
            all_columns = list(X.columns)
        except AttributeError as e:
            raise ValueError(
                "Specifying the columns using strings is only "
                "supported for pandas DataFrames"
            ) from e
        if isinstance(key, str):
            columns = [key]
        elif isinstance(key, slice):
            start, stop = key.start, key.stop
            if start is not None:
                start = all_columns.index(start)
            if stop is not None:
                # pandas indexing with strings is endpoint included
                stop = all_columns.index(stop) + 1
            else:
                stop = n_channels + 1
            return list(range(n_channels)[slice(start, stop)])
        else:
            columns = list(key)

        return [all_columns.index(col) for col in columns]
    else:
        raise ValueError(
            "No valid specification of the columns. Only a "
            "scalar, list or slice of all integers or all "
            "strings, or boolean mask is allowed"
        )


def _get_channel(X, key):
    """
    Get time series channel(s) from input data X.

    Supported input types (X): numpy arrays

    Supported key types (key):
    - scalar: output is 1D
    - lists, slices, boolean masks: output is 2D
    - callable that returns any of the above

    Supported key data types:

    - integer or boolean mask (positional):
        - supported for arrays and sparse matrices
    - string (key-based):
        - only supported for dataframes
        - So no keys other than strings are allowed (while in principle you
          can use any hashable object as key).

    """
    # check whether we have string channel names or integers
    if _check_key_type(key, int):
        channel_names = False
    elif hasattr(key, "dtype") and np.issubdtype(key.dtype, np.bool_):
        # boolean mask
        channel_names = True
    else:
        raise ValueError(
            "No valid specification of the channels. Only a "
            "scalar, list or slice of all integers or all "
            "strings, or boolean mask is allowed"
        )

    if isinstance(key, (int, str)):
        key = [key]

    if not channel_names:
        return X[:, key] if isinstance(X, np.ndarray) else X.iloc[:, key]
    if not isinstance(X, pd.DataFrame):
        raise ValueError(
            f"X must be a pd.DataFrame if channel names are "
            f"specified, but found: {type(X)}"
        )
    return X.loc[:, key]


def _check_key_type(key, superclass):
    """
    Check that scalar, list or slice is of a certain type.

    This is only used in _get_channel and _get_channel_indices to check
    if the `key` (channel specification) is fully integer or fully string-like.

    Parameters
    ----------
    key : scalar, list, slice, array-like
        The channel specification to check
    superclass : int or str
        The type for which to check the `key`

    """
    if isinstance(key, superclass):
        return True
    if isinstance(key, slice):
        return isinstance(key.start, (superclass, type(None))) and isinstance(
            key.stop, (superclass, type(None))
        )
    if isinstance(key, list):
        return all(isinstance(x, superclass) for x in key)
    if hasattr(key, "dtype"):
        if superclass is int:
            return key.dtype.kind == "i"
        else:
            # superclass = str
            return key.dtype.kind in ("O", "U", "S")
    return False
